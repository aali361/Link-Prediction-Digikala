"""
This is my final project in Complex Networks course in Amirkabir university
In this project we use various techniques for link prediction in Digikala dataset
Digikala has published some of it's datasets for researchers
We use Orders table of this dataset that have 200k order in it
Our result shows that Machnie Learning techniques have best accuracy in this task
Our source code and result are published for additional research and good purposes
You can use our code in Colab easily
"""

# -*- coding: utf-8 -*-
"""complex_project

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import collections

# For sorting dict
import operator

# For sampling
import random

# Pandas
from pandas import read_excel

# Biparitie Algorithms
import networkx as nx
from networkx.algorithms import bipartite

# For split data to train&test
from sklearn.model_selection import train_test_split

!wget http://deepyeti.ucsd.edu/jianmo/amazon/categoryFilesSmall/Software.csv
!wget https://mo-ghorbani.ir/orders.csv

"""# Degree Distribution of Dataset



## Amazon Degree Distribution
"""

file_name = 'Software.csv' 
df_amazon = pd.read_csv(file_name)
df_amazon.columns = ['product_id', 'user_id', 'score']

# Shuffling dataframe and return 25% of data
df_amazon = df_amazon.sample(frac=1).reset_index(drop=True)

bipartiteAmazon = nx.Graph()
for key, row in list(enumerate(range(df_amazon.shape[0])))[:]:
  user = df_amazon['user_id'][key]
  product = df_amazon['product_id'][key]
  bipartiteAmazon.add_node(user, bipartite=0)
  bipartiteAmazon.add_node(product, bipartite=1)
  bipartiteAmazon.add_edge(user, product)
print('all nodes in Amazon: ' + str(bipartiteAmazon.number_of_nodes()))
print('all edges in Amazon: ' + str(bipartiteAmazon.number_of_edges()))

degree_sequence=sorted(dict(nx.degree(bipartiteAmazon)).values(),reverse=True) 
plt.loglog(degree_sequence,marker='*')
plt.xlabel('Count')
plt.ylabel('Degree') 
plt.title('Amazon')
plt.figure(figsize=(10,8))
plt.show()

file_name = 'orders.csv' 
df_digikala = pd.read_csv(file_name)
df_digikala.columns = ['ID_Order','ID_Customer','ID_Item','DateTime_CartFinalize','Amount_Gross_Order','city_name_fa','Quantity_item']

# Shuffling dataframe and return 25% of data
df_digikala = df_digikala.sample(frac=1).reset_index(drop=True)
# df_digikala.shape




"""## Digikala Degree Distribution"""

bipartiteAmazon = nx.Graph()
for key, row in list(enumerate(range(df_digikala.shape[0])))[:]:
  user = df_digikala['ID_Customer'][key]
  product = df_digikala['ID_Item'][key]
  bipartiteAmazon.add_node(user, bipartite=0)
  bipartiteAmazon.add_node(product, bipartite=1)
  bipartiteAmazon.add_edge(user, product)
print('all nodes: ' + str(bipartiteAmazon.number_of_nodes()))
print('all edges: ' + str(bipartiteAmazon.number_of_edges()))

degree_sequence=sorted(dict(nx.degree(bipartiteAmazon)).values(),reverse=True) 
plt.loglog(degree_sequence,marker='*')
plt.xlabel('Count')
plt.ylabel('Degree') 
plt.title('Digikala')
plt.show()

"""# Data Preparation"""

file_name = 'orders.csv' # name of your excel file
df = pd.read_csv(file_name)
df.columns = ['ID_Order','ID_Customer','ID_Item','DateTime_CartFinalize','Amount_Gross_Order','city_name_fa','Quantity_item']

# Shuffling dataframe and return 25% of data
df = df.sample(frac=0.25).reset_index(drop=True)
# df.shape

"""## Creating Bipartie Graph"""

bipartiteG = nx.Graph()
for key, row in list(enumerate(range(df.shape[0])))[:]:
  user = df['ID_Customer'][key]
  product = df['ID_Item'][key]
  bipartiteG.add_node(user, bipartite=0)
  bipartiteG.add_node(product, bipartite=1)
  bipartiteG.add_edge(user, product)
print('all nodes: ' + str(bipartiteG.number_of_nodes()))
print('all edges: ' + str(bipartiteG.number_of_edges()))

"""## Creating Projected Graph"""

item_nodes = {n for n, d in bipartiteG.nodes(data=True) if d['bipartite']==1}
G_items = bipartite.projected_graph(bipartiteG, item_nodes)
print("Number of nodes: " + str(G_items.number_of_nodes()))
print("Number of edges: " + str(G_items.number_of_edges()))

"""## Removing Nodes with degree less than 3"""

remove = [node for node,degree in list(G_items.degree()) if degree < 3]
G_main = G_items.copy()
G_main.remove_nodes_from(remove)
print("Number of nodes: " + str(G_main.number_of_nodes()))
print("Number of edges: " + str(G_main.number_of_edges()))

"""## Removing 10 Percent of Edges"""

percentOfTest = 0.1
numberOfEdges = int(float(percentOfTest) * G_main.number_of_edges())
removed_edges = random.sample(G_main.edges, numberOfEdges)
G_train = G_main.copy()
G_train.remove_edges_from(removed_edges)
print("Number of edges that must be removed: " + str(numberOfEdges))
print("Number of nodes: " + str(G_train.number_of_nodes()))
print("Number of edges: " + str(G_train.number_of_edges()))

"""## Evaluation Function"""

def evaluate(list_of_predicted_edges, top_k):
  list_of_predicted_edges = list_of_predicted_edges[:top_k]
  intersection_with_test_edges = set([tuple(sorted(elem)) for elem in removed_edges]) & set([tuple(sorted(elem)) for elem in list_of_predicted_edges])
  print('Evaluation for k='+str(top_k))
  print('----->  Number of correct links is: ' + str(len(intersection_with_test_edges)))
  print('----->  P@'+str(top_k)+' is: '+str(len(intersection_with_test_edges)/top_k))

"""# Approach 1: Using Similarity Based Methods"""

def predicted_links_method1(method):
  train_nodes = list(G_train.nodes())
  all_scores = []
  for key1 ,i in enumerate(train_nodes):
    temp_score = {}
    for key2, j in enumerate(train_nodes):
      if key1 != key2:
        if not G_train.has_edge(i,j):
          score = 0
          if method == 'jaccard':
            score =  list(nx.jaccard_coefficient(G_train, [(i, j)]))[0][2]
          elif method == 'adamic_adar':
            score = list(nx.adamic_adar_index(G_train, [(i, j)]))[0][2]
          elif method == 'prefer':
            score = list(nx.preferential_attachment(G_train, [(i, j)]))[0][2]
          elif method == 'cn':
            score = len(list(nx.common_neighbors(G_train, i, j)))
          elif method == 'reso_alloc':
            score = list(nx.resource_allocation_index(G_train, [(i, j)]))[0][2]
          if score != 0:
            temp_score.update({train_nodes[key2]: score})
    sorted_score = sorted(temp_score.items(), key=operator.itemgetter(1))

    sorted_score.reverse()
    for node in sorted_score:
      all_scores.append((train_nodes[key1], node[0], node[1]))
  sorted_by_third = sorted(all_scores, key=lambda tup: tup[2])
  sorted_by_third.reverse()
  sorted_by_third = sorted_by_third

  list_of_predicted_edges = []
  for i in sorted_by_third:
    list_of_predicted_edges.append((i[0], i[1]))
  return list_of_predicted_edges

list_of_predicted_edges = predicted_links_method1('cn')
print('Method: cn')
evaluate(list_of_predicted_edges, 20)
evaluate(list_of_predicted_edges, 50)
evaluate(list_of_predicted_edges, 100)

list_of_predicted_edges = predicted_links_method1('jaccard')
print('Method: jaccard')
evaluate(list_of_predicted_edges, 20)
evaluate(list_of_predicted_edges, 50)
evaluate(list_of_predicted_edges, 100)

list_of_predicted_edges = predicted_links_method1('adamic_adar')
print('Method: adamic_adar')
evaluate(list_of_predicted_edges, 20)
evaluate(list_of_predicted_edges, 50)
evaluate(list_of_predicted_edges, 100)

list_of_predicted_edges = predicted_links_method1('prefer')
print('Method: prefer')
evaluate(list_of_predicted_edges, 20)
evaluate(list_of_predicted_edges, 50)
evaluate(list_of_predicted_edges, 100)

list_of_predicted_edges = predicted_links_method1('reso_alloc')
print('Method: reso_alloc')
evaluate(list_of_predicted_edges, 20)
evaluate(list_of_predicted_edges, 50)
evaluate(list_of_predicted_edges, 100)














"""# Approach2: Using Matrix Completion

## Matrix Completion Function
"""

import numpy as np
from numpy.linalg import norm, svd

def inexact_augmented_lagrange_multiplier(X, lmbda=.01, tol=1e-3,
                                          maxiter=100, verbose=True):
    """
    Inexact Augmented Lagrange Multiplier
    """
    Y = X
    norm_two = norm(Y.ravel(), 2)
    norm_inf = norm(Y.ravel(), np.inf) / lmbda
    dual_norm = np.max([norm_two, norm_inf])
    Y = Y / dual_norm
    A = np.zeros(Y.shape)
    E = np.zeros(Y.shape)
    dnorm = norm(X, 'fro')
    mu = 1.25 / norm_two
    rho = 1.5
    sv = 10.
    n = Y.shape[0]
    itr = 0
    while True:
        Eraw = X - A + (1 / mu) * Y
        Eupdate = np.maximum(Eraw - lmbda / mu, 0) + np.minimum(Eraw + lmbda / mu, 0)
        U, S, V = svd(X - Eupdate + (1 / mu) * Y, full_matrices=False)
        svp = (S > 1 / mu).shape[0]
        if svp < sv:
            sv = np.min([svp + 1, n])
        else:
            sv = np.min([svp + round(.05 * n), n])
        Aupdate = np.dot(np.dot(U[:, :svp], np.diag(S[:svp] - 1 / mu)), V[:svp, :])
        A = Aupdate
        E = Eupdate
        Z = X - A - E
        Y = Y + mu * Z
        mu = np.min([mu * rho, mu * 1e7])
        itr += 1
        if ((norm(Z, 'fro') / dnorm) < tol) or (itr >= maxiter):
            break
    if verbose:
        print("Finished at iteration %d" % (itr))  
    return A, E

"""## Adjancy Matrix Preparation"""

matrix_train = nx.to_numpy_matrix(G_train)
matrix_train.shape
L, S = inexact_augmented_lagrange_multiplier(matrix_train)
L_sum = np.add(L, L.transpose())
row, column = L_sum.shape

def predicted_links_method2():
  all_scores = []
  train_nodes = list(G_train.nodes())
  for x in range(row)[:]:
      for y in range(column):
          if matrix_train[x, y]== 0 and L_sum[x, y] !=0:
            all_scores.append((train_nodes[x], train_nodes[y], L_sum[x, y]))
      all_scores = sorted(all_scores, key=lambda tup: tup[2])
      all_scores.reverse()
      all_scores = all_scores[:]

  list_of_predicted_edges = []
  for i in all_scores:
    list_of_predicted_edges.append((i[0], i[1]))
  return list_of_predicted_edges

list_of_predicted_edges = predicted_links_method2()
evaluate(list_of_predicted_edges, 20)
evaluate(list_of_predicted_edges, 50)
evaluate(list_of_predicted_edges, 100)

"""# Approach3: Using SVM && Logistic Regression

## Prepare features and labels
"""

G_train2 = G_train.copy()
nonEdgesList = random.sample(list(nx.non_edges(G_train2)), G_train2.number_of_edges())
G_train2.add_edges_from(nonEdgesList)
print("Number of edges: " + str(G_train2.number_of_edges()))

feature_list = []
existings = []
for edge in list(G_train2.edges())[:]:
  temp = []
  x_node = edge[0]
  y_node = edge[1]
  score_jaccard =  list(nx.jaccard_coefficient(G_train2, [(x_node, y_node)]))[0][2]
  score_cn = len(list(nx.common_neighbors(G_train2, x_node, y_node)))
  score_reso = list(nx.resource_allocation_index(G_train2, [(x_node, y_node)]))[0][2]
  x_degree = G_train2.degree(x_node)
  y_degree = G_train2.degree(y_node)
  feature_list.append([x_degree, y_degree, score_jaccard, score_cn, score_reso])
  if edge in nonEdgesList:
    existings.append(-1)
  else:
    existings.append(1)







"""## Using SVM"""

from sklearn import svm
clf_svm = svm.SVC(kernel= 'linear', probability= True)
clf_svm.fit(feature_list, existings)

def predicted_links_method3_svm():
  non_edges_graph = list(nx.non_edges(G_train2))
  all_scores = []
  for key, edge in enumerate(non_edges_graph[:]):
    i = edge[0]
    j = edge[1]
    score_jaccard =  list(nx.jaccard_coefficient(G_train2, [(i, j)]))[0][2]
    score_cn = len(list(nx.common_neighbors(G_train2, i, j)))
    score_reso = list(nx.resource_allocation_index(G_train2, [(i, j)]))[0][2]
    x_degree = G_train2.degree(i)
    y_degree = G_train2.degree(j)
    feature_list = []
    feature_list.append([x_degree, y_degree, score_jaccard, score_cn, score_reso])
    flag = clf_svm.predict(feature_list)[0]
    if flag == 1:
      score = clf_svm.predict_proba(feature_list)[0][1]
      all_scores.append((i, j, score))

  all_scores = sorted(all_scores, key=lambda tup: tup[2])
  all_scores.reverse()
  list_of_predicted_edges = []
  for i in all_scores:
    list_of_predicted_edges.append((i[0], i[1]))
  return list_of_predicted_edges

predicted_links_svm = predicted_links_method3_svm()

evaluate(predicted_links_svm, 40)
evaluate(predicted_links_svm, 50)
evaluate(predicted_links_svm, 60)
evaluate(predicted_links_svm, 70)
evaluate(predicted_links_svm, 80)
evaluate(predicted_links_svm, 90)
evaluate(predicted_links_svm, 100)







"""## Using LR"""

from sklearn.linear_model import LogisticRegression
clf_logis = LogisticRegression(random_state=0).fit(feature_list, existings)

def predicted_links_method3_logistic():
  non_edges_graph = list(nx.non_edges(G_train2))
  all_scores = []
  for key, edge in enumerate(non_edges_graph[:]):
    i = edge[0]
    j = edge[1]
    score_jaccard =  list(nx.jaccard_coefficient(G_train2, [(i, j)]))[0][2]
    score_cn = len(list(nx.common_neighbors(G_train2, i, j)))
    score_reso = list(nx.resource_allocation_index(G_train2, [(i, j)]))[0][2]
    x_degree = G_train2.degree(i)
    y_degree = G_train2.degree(j)
    feature_list = []
    feature_list.append([x_degree, y_degree, score_jaccard, score_cn, score_reso])
    flag = clf_logis.predict(feature_list)[0]
    if flag == 1:
      score = clf_logis.predict_proba(feature_list)[0][1]
      all_scores.append((i, j, score))

  all_scores = sorted(all_scores, key=lambda tup: tup[2])
  all_scores.reverse()
  list_of_predicted_edges = []
  for i in all_scores:
    list_of_predicted_edges.append((i[0], i[1]))
  return list_of_predicted_edges

predicted_links_logis = predicted_links_method3_logistic()

evaluate(predicted_links_logis, 20)
evaluate(predicted_links_logis, 50)
evaluate(predicted_links_logis, 100)













"""# Result"""

# libraries
import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
 
# Data

x = [5, 10, 20, 30, 40, 50, 60, 70, 80, 90, 100]
CN = [0.8, 0.7, 0.65, 0.5, 0.525, 0.52, 0.5, 0.528, 0.5125, 0.477, 0.5]
JA = [0.4, 0.3, 0.35, 0.36, 0.425, 0.38, 0.383, 0.357, 0.425, 0.433, 0.42]
AA = [0.6, 0.5, 0.55, 0.5, 0.5, 0.5, 0.516, 0.5, 0.5, 0.48, 0.49]
PA = [0.0, 0.0, 0.05, 0.067, 0.05, 0.04, 0.033, 0.0285, 0.025, 0.022, 0.02]
RA = [0.6, 0.5, 0.45, 0.46, 0.475, 0.48, 0.485, 0.483, 0.487, 0.477, 0.47]
MC = [0.2, 0.1, 0.05, 0.03, 0.05, 0.08, 0.1, 0.1, 0.1, 0.1, 0.11]
SVM = [1, 1, 0.95, 0.96, 0.925, 0.88, 0.90, 0.85, 0.83, 0.83, 0.82]
LR = [0.6, 0.8, 0.85, 0.8, 0.85, 0.88, 0.9, 0.85, 0.83, 0.8, 0.82]
df=pd.DataFrame({'x': x, 'CN': CN, 'JA':JA, 'AA': AA, 'PA':PA, 'RA':RA, 'MC':MC, 'SVM':SVM, 'LR':LR})
plt.figure(figsize=(10,8))
plt.plot( 'x', 'SVM', data=df, marker='o', markerfacecolor='#4840C5', markersize=6, color='#D3D5E5', linewidth=2)
plt.plot( 'x', 'LR', data=df, marker='o', markerfacecolor='red', markersize=6, color='#FFD2D2', linewidth=2)
plt.plot( 'x', 'MC', data=df, marker='o', markerfacecolor='blue', markersize=6, color='skyblue', linewidth=2)
plt.plot( 'x', 'JA', data=df, marker='o', markerfacecolor='#FC7D0E', markersize=6, color='#FFE7D2', linewidth=2)
plt.plot( 'x', 'PA', data=df, marker='o', markerfacecolor='green', markersize=6, color='#CBFFE4', linewidth=2)
plt.xlabel('K')
plt.ylabel('Prec@k')
plt.title('Prec@k vs K') 
plt.legend()
plt.show()

# libraries
import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
 
# Data for method1

x = [3, 5, 10]
train_map = [0.003, 0.012, 0.006]
test_map = [0.022, 0.044, 0.034]
JA = [0.4, 0.3, 0.35, 0.36, 0.425, 0.38, 0.383, 0.357, 0.425, 0.433, 0.42]
AA = [0.6, 0.5, 0.55, 0.5, 0.5, 0.5, 0.516, 0.5, 0.5, 0.48, 0.49]
PA = [0.0, 0.0, 0.05, 0.067, 0.05, 0.04, 0.033, 0.0285, 0.025, 0.022, 0.02]
RA = [0.6, 0.5, 0.45, 0.46, 0.475, 0.48, 0.485, 0.483, 0.487, 0.477, 0.47]
MC = [0.2, 0.1, 0.05, 0.03, 0.05, 0.08, 0.1, 0.1, 0.1, 0.1, 0.11]
SVM = [1, 1, 0.95, 0.96, 0.925, 0.88, 0.90, 0.85, 0.83, 0.83, 0.82]
LR = [0.6, 0.8, 0.85, 0.8, 0.85, 0.88, 0.9, 0.85, 0.83, 0.8, 0.82]
df=pd.DataFrame({'x': x, 'CN': CN, 'JA':JA, 'AA': AA, 'PA':PA, 'RA':RA, 'MC':MC, 'SVM':SVM, 'LR':LR})
plt.figure(figsize=(10,8))
plt.plot( 'x', 'SVM', data=df, marker='o', markerfacecolor='#4840C5', markersize=6, color='#D3D5E5', linewidth=2)
plt.plot( 'x', 'LR', data=df, marker='o', markerfacecolor='red', markersize=6, color='#FFD2D2', linewidth=2)
plt.plot( 'x', 'MC', data=df, marker='o', markerfacecolor='blue', markersize=6, color='skyblue', linewidth=2)
plt.plot( 'x', 'JA', data=df, marker='o', markerfacecolor='#FC7D0E', markersize=6, color='#FFE7D2', linewidth=2)
plt.plot( 'x', 'PA', data=df, marker='o', markerfacecolor='green', markersize=6, color='#CBFFE4', linewidth=2)
plt.xlabel('K')
plt.ylabel('Prec@k')
plt.title('Prec@k vs K') 
plt.legend()
plt.show()
